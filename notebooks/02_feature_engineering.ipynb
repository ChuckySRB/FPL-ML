{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPL Feature Engineering & Preprocessing\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Feature engineering for player performance prediction\n",
    "2. Creating rolling averages and lag features\n",
    "3. Form metrics and temporal features\n",
    "4. Data preprocessing pipeline\n",
    "5. Train/test splitting strategies\n",
    "6. Preparing ML-ready datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project modules\n",
    "from src.preprocessing import (\n",
    "    FPLDataLoader,\n",
    "    FPLFeatureEngineer,\n",
    "    FPLPreprocessor,\n",
    "    prepare_training_data,\n",
    "    get_feature_columns_by_type,\n",
    "    POSITIONS\n",
    ")\n",
    "\n",
    "# Settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loader\n",
    "loader = FPLDataLoader()\n",
    "\n",
    "# Load data for a specific season\n",
    "SEASON = '2023-24'  # Change as needed\n",
    "\n",
    "print(f\"Loading data for season: {SEASON}\")\n",
    "gameweeks_df = loader.load_gameweeks(SEASON)\n",
    "players_df = loader.load_players(SEASON)\n",
    "teams_df = loader.load_teams(SEASON)\n",
    "\n",
    "print(f\"\\n✓ Loaded:\")\n",
    "print(f\"  Gameweeks: {len(gameweeks_df):,} records\")\n",
    "print(f\"  Players: {len(players_df):,}\")\n",
    "print(f\"  Teams: {len(teams_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check initial data\n",
    "print(\"Gameweek data columns:\")\n",
    "print(gameweeks_df.columns.tolist())\n",
    "print(f\"\\nShape: {gameweeks_df.shape}\")\n",
    "print(f\"\\nSample:\")\n",
    "gameweeks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FPLFeatureEngineer()\n",
    "\n",
    "# Ensure data is sorted\n",
    "gameweeks_df = gameweeks_df.sort_values(['element', 'round'])\n",
    "\n",
    "print(\"Creating all features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create all features\n",
    "features_df = engineer.create_all_features(\n",
    "    gameweeks_df,\n",
    "    teams_df=teams_df,\n",
    "    players_df=players_df\n",
    ")\n",
    "\n",
    "print(f\"\\nFeatures created!\")\n",
    "print(f\"  Original columns: {len(gameweeks_df.columns)}\")\n",
    "print(f\"  With features: {len(features_df.columns)}\")\n",
    "print(f\"  New features: {len(features_df.columns) - len(gameweeks_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Examine Created Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features\n",
    "feature_types = get_feature_columns_by_type(features_df)\n",
    "\n",
    "print(\"FEATURE GROUPS\")\n",
    "print(\"=\"*60)\n",
    "for group, features in feature_types.items():\n",
    "    if features:\n",
    "        print(f\"\\n{group.upper()} ({len(features)} features):\")\n",
    "        print(f\"  {', '.join(features[:10])}\")\n",
    "        if len(features) > 10:\n",
    "            print(f\"  ... and {len(features) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of feature-engineered data\n",
    "print(\"Sample of engineered features:\")\n",
    "sample_cols = [\n",
    "    'element', 'round', 'total_points',\n",
    "    'total_points_rolling_3', 'total_points_rolling_5',\n",
    "    'form_weighted', 'consistency_5', 'form_trend',\n",
    "    'goal_involvement', 'points_per_90'\n",
    "]\n",
    "existing_cols = [col for col in sample_cols if col in features_df.columns]\n",
    "features_df[existing_cols].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Rolling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample player to visualize\n",
    "if 'element' in features_df.columns:\n",
    "    sample_player_id = features_df['element'].value_counts().index[0]\n",
    "    player_data = features_df[features_df['element'] == sample_player_id].copy()\n",
    "    \n",
    "    if len(player_data) > 10:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        \n",
    "        # Rolling points\n",
    "        if all(col in player_data.columns for col in ['round', 'total_points', 'total_points_rolling_3', 'total_points_rolling_5']):\n",
    "            axes[0, 0].plot(player_data['round'], player_data['total_points'], \n",
    "                           'o-', alpha=0.5, label='Actual', markersize=4)\n",
    "            axes[0, 0].plot(player_data['round'], player_data['total_points_rolling_3'],\n",
    "                           '-', linewidth=2, label='3-game avg')\n",
    "            axes[0, 0].plot(player_data['round'], player_data['total_points_rolling_5'],\n",
    "                           '-', linewidth=2, label='5-game avg')\n",
    "            axes[0, 0].set_title(f'Player {sample_player_id}: Points & Rolling Averages', fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('Gameweek')\n",
    "            axes[0, 0].set_ylabel('Points')\n",
    "            axes[0, 0].legend()\n",
    "            axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Form metrics\n",
    "        if all(col in player_data.columns for col in ['round', 'form_weighted', 'form_trend']):\n",
    "            ax2 = axes[0, 1]\n",
    "            ax2.plot(player_data['round'], player_data['form_weighted'],\n",
    "                    '-o', linewidth=2, markersize=4, label='Weighted Form')\n",
    "            ax2.set_ylabel('Weighted Form', color='blue')\n",
    "            ax2.tick_params(axis='y', labelcolor='blue')\n",
    "            \n",
    "            ax2_twin = ax2.twinx()\n",
    "            ax2_twin.plot(player_data['round'], player_data['form_trend'],\n",
    "                         '-s', color='red', linewidth=2, markersize=4, label='Form Trend')\n",
    "            ax2_twin.set_ylabel('Form Trend', color='red')\n",
    "            ax2_twin.tick_params(axis='y', labelcolor='red')\n",
    "            ax2_twin.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "            \n",
    "            ax2.set_title('Form Metrics', fontweight='bold')\n",
    "            ax2.set_xlabel('Gameweek')\n",
    "            ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # Goal involvement\n",
    "        if all(col in player_data.columns for col in ['round', 'goals_scored', 'assists', 'goal_involvement']):\n",
    "            axes[1, 0].bar(player_data['round'], player_data['goals_scored'],\n",
    "                          alpha=0.7, label='Goals', color='red')\n",
    "            axes[1, 0].bar(player_data['round'], player_data['assists'],\n",
    "                          bottom=player_data['goals_scored'],\n",
    "                          alpha=0.7, label='Assists', color='blue')\n",
    "            axes[1, 0].plot(player_data['round'], \n",
    "                           player_data.get('goal_involvement_rolling_3', player_data['goal_involvement']),\n",
    "                           'k-', linewidth=2, label='3-game avg')\n",
    "            axes[1, 0].set_title('Goal Involvement', fontweight='bold')\n",
    "            axes[1, 0].set_xlabel('Gameweek')\n",
    "            axes[1, 0].set_ylabel('Count')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Minutes played\n",
    "        if 'minutes' in player_data.columns:\n",
    "            axes[1, 1].bar(player_data['round'], player_data['minutes'],\n",
    "                          alpha=0.7, color='green')\n",
    "            axes[1, 1].axhline(90, color='red', linestyle='--', label='Full 90')\n",
    "            axes[1, 1].axhline(60, color='orange', linestyle='--', label='60 min')\n",
    "            axes[1, 1].set_title('Minutes Played', fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Gameweek')\n",
    "            axes[1, 1].set_ylabel('Minutes')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Not enough data to visualize\")\n",
    "else:\n",
    "    print(\"Element column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "print(\"Preparing training data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove first few games per player (incomplete rolling features)\n",
    "training_ready_df = prepare_training_data(\n",
    "    features_df,\n",
    "    target_col='total_points',\n",
    "    drop_first_n_games=5\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training data prepared\")\n",
    "print(f\"  Original records: {len(features_df):,}\")\n",
    "print(f\"  Training-ready records: {len(training_ready_df):,}\")\n",
    "print(f\"  Removed: {len(features_df) - len(training_ready_df):,} ({(1 - len(training_ready_df)/len(features_df))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_pct = (training_ready_df.isnull().sum() / len(training_ready_df) * 100)\n",
    "missing_cols = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    print(missing_cols.head(10))\n",
    "else:\n",
    "    print(\"\\n✓ No missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = FPLPreprocessor(scaler_type='standard')\n",
    "\n",
    "# Run full pipeline\n",
    "print(\"\\nRunning preprocessing pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processed_data = preprocessor.prepare_for_training(\n",
    "    training_ready_df,\n",
    "    target_col='total_points',\n",
    "    temporal_split=True,  # Use temporal split for time series\n",
    "    test_rounds=5,  # Last 5 gameweeks for testing\n",
    "    scale=True,\n",
    "    handle_missing=True\n",
    ")\n",
    "\n",
    "# Extract data\n",
    "X_train = processed_data['X_train']\n",
    "X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "y_test = processed_data['y_test']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL DATASETS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTarget distribution (train):\")\n",
    "print(y_train.describe())\n",
    "print(f\"\\nTarget distribution (test):\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train/test split\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Target distribution\n",
    "axes[0].hist(y_train, bins=30, alpha=0.7, label='Train', edgecolor='black')\n",
    "axes[0].hist(y_test, bins=30, alpha=0.7, label='Test', edgecolor='black')\n",
    "axes[0].set_title('Target Distribution (Total Points)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Points')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Dataset sizes\n",
    "sizes = [len(X_train), len(X_test)]\n",
    "labels = ['Train', 'Test']\n",
    "axes[1].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90,\n",
    "           colors=['steelblue', 'orange'])\n",
    "axes[1].set_title('Train/Test Split', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation with target\n",
    "print(\"Top features correlated with target:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Combine X and y for correlation analysis\n",
    "train_with_target = X_train.copy()\n",
    "train_with_target['target'] = y_train.values\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = train_with_target.corr()['target'].drop('target').abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 features by correlation:\")\n",
    "print(correlations.head(20))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlations.head(20).plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "plt.title('Top 20 Features by Correlation', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance by group\n",
    "feature_groups = get_feature_columns_by_type(X_train)\n",
    "\n",
    "print(\"\\nAverage correlation by feature group:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "group_correlations = {}\n",
    "for group, features in feature_groups.items():\n",
    "    if features:\n",
    "        avg_corr = correlations[correlations.index.isin(features)].mean()\n",
    "        group_correlations[group] = avg_corr\n",
    "        print(f\"{group:15s}: {avg_corr:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd.Series(group_correlations).sort_values().plot(kind='barh', color='green', alpha=0.7)\n",
    "plt.xlabel('Average Absolute Correlation')\n",
    "plt.title('Feature Group Importance', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "from configs.config import PROCESSED_DATA_DIR\n",
    "\n",
    "output_dir = PROCESSED_DATA_DIR / SEASON\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save train/test sets\n",
    "X_train.to_csv(output_dir / 'X_train.csv', index=False)\n",
    "X_test.to_csv(output_dir / 'X_test.csv', index=False)\n",
    "y_train.to_csv(output_dir / 'y_train.csv', index=False, header=['total_points'])\n",
    "y_test.to_csv(output_dir / 'y_test.csv', index=False, header=['total_points'])\n",
    "\n",
    "# Save feature-engineered full dataset\n",
    "training_ready_df.to_csv(output_dir / 'features_complete.csv', index=False)\n",
    "\n",
    "print(f\"✓ Data saved to: {output_dir}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  - X_train.csv ({X_train.shape})\")\n",
    "print(f\"  - X_test.csv ({X_test.shape})\")\n",
    "print(f\"  - y_train.csv ({len(y_train)} records)\")\n",
    "print(f\"  - y_test.csv ({len(y_test)} records)\")\n",
    "print(f\"  - features_complete.csv ({training_ready_df.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing pipeline\n",
    "from configs.config import MODELS_DIR\n",
    "\n",
    "pipeline_path = MODELS_DIR / f'preprocessor_{SEASON}.pkl'\n",
    "preprocessor.save_pipeline(str(pipeline_path))\n",
    "\n",
    "print(f\"\\n✓ Preprocessing pipeline saved to: {pipeline_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Features Created:\n",
    "1. **Rolling Features**: Moving averages (3, 5, 10 games) for key metrics\n",
    "2. **Lag Features**: Previous gameweek values (1, 2, 3 lags)\n",
    "3. **Form Features**: Weighted form, consistency, trend\n",
    "4. **Attacking Features**: Goal involvement, xG, xA, creativity\n",
    "5. **Defensive Features**: Clean sheets, goals conceded, saves\n",
    "6. **Opponent Features**: Difficulty ratings, team strength\n",
    "7. **Home/Away Features**: Location-specific performance\n",
    "8. **Value Features**: Points per cost, transfer trends\n",
    "\n",
    "### Data Ready For:\n",
    "- Training ML models (Random Forest, XGBoost, LightGBM)\n",
    "- Position-specific predictions\n",
    "- Temporal validation\n",
    "- Comparison with research paper benchmarks\n",
    "\n",
    "### Next Steps:\n",
    "1. Train baseline models\n",
    "2. Hyperparameter tuning\n",
    "3. Model evaluation and comparison\n",
    "4. Feature selection and importance analysis\n",
    "5. Deploy for predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
