{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation\n",
    "\n",
    "**Reference:** Paper 1 — OpenFPL (Groos, 2025)\n",
    "\n",
    "Every model run is automatically logged to `outputs/results/experiment_runs.jsonl`.\n",
    "At the end, `tracker.summary()` shows the full comparison table across all runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from src.preprocessing import TIER1_FEATURES, TIER2_FEATURES\n",
    "from src.evaluation import ExperimentTracker\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# One tracker for the whole notebook session\n",
    "tracker = ExperimentTracker()\n",
    "\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/processed/tier2_2022-23_to_2023-24')\n",
    "\n",
    "X_train = pd.read_csv(data_dir / 'X_train.csv')\n",
    "X_test  = pd.read_csv(data_dir / 'X_test.csv')\n",
    "y_train = pd.read_csv(data_dir / 'y_train.csv')['total_points']\n",
    "y_test  = pd.read_csv(data_dir / 'y_test.csv')['total_points']\n",
    "\n",
    "# Full DataFrames for position / category grouping\n",
    "train_full = pd.read_csv(data_dir / 'train_full.csv')\n",
    "test_full  = pd.read_csv(data_dir / 'test_full.csv')\n",
    "\n",
    "# Positions array — always stays aligned with X_test / y_test\n",
    "test_positions = test_full['position_label'].values\n",
    "\n",
    "# Available feature columns by tier\n",
    "tier1_cols = [f for f in TIER1_FEATURES if f in X_train.columns]\n",
    "tier2_cols = list(X_train.columns)   # already Tier 2\n",
    "\n",
    "print(f'Train  : {X_train.shape[0]:,} samples')\n",
    "print(f'Test   : {X_test.shape[0]:,} samples')\n",
    "print(f'Tier 1 : {len(tier1_cols)} features')\n",
    "print(f'Tier 2 : {len(tier2_cols)} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Baseline — Last 5 Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_preds = X_test['form_last_5'].values\n",
    "\n",
    "tracker.log(\n",
    "    name='Naive: Last 5 Avg',\n",
    "    y_true=y_test, y_pred=naive_preds, positions=test_positions,\n",
    "    config={\n",
    "        'model': 'Naive',\n",
    "        'features': 'form_last_5 only',\n",
    "        'n_features': 1,\n",
    "        'train_seasons': '2022-23',\n",
    "        'test_season': '2023-24',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Linear Regression — Tier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_tr_t1 = scaler.fit_transform(X_train[tier1_cols])\n",
    "X_te_t1 = scaler.transform(X_test[tier1_cols])\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_tr_t1, y_train)\n",
    "lr_preds = lr.predict(X_te_t1)\n",
    "\n",
    "tracker.log(\n",
    "    name='LinearRegression Tier1',\n",
    "    y_true=y_test, y_pred=lr_preds, positions=test_positions,\n",
    "    config={\n",
    "        'model': 'LinearRegression',\n",
    "        'features': 'Tier 1',\n",
    "        'n_features': len(tier1_cols),\n",
    "        'train_seasons': '2022-23',\n",
    "        'test_season': '2023-24',\n",
    "        'params': {'fit_intercept': True, 'scaling': 'StandardScaler'}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost — Experiment with Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run 1: XGBoost default-ish ---\n",
    "xgb_params = dict(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                  subsample=1.0, colsample_bytree=1.0, random_state=42, n_jobs=-1)\n",
    "\n",
    "xgb1 = XGBRegressor(**xgb_params)\n",
    "xgb1.fit(X_train, y_train)\n",
    "preds = xgb1.predict(X_test)\n",
    "\n",
    "tracker.log(\n",
    "    name='XGBoost n100 d5 lr0.1',\n",
    "    y_true=y_test, y_pred=preds, positions=test_positions,\n",
    "    config={\n",
    "        'model': 'XGBoost',\n",
    "        'features': 'Tier 2',\n",
    "        'n_features': len(tier2_cols),\n",
    "        'train_seasons': '2022-23',\n",
    "        'test_season': '2023-24',\n",
    "        'params': xgb_params,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run 2: XGBoost more trees, lower lr ---\n",
    "xgb_params2 = dict(n_estimators=300, max_depth=5, learning_rate=0.05,\n",
    "                   subsample=0.85, colsample_bytree=0.85, random_state=42, n_jobs=-1)\n",
    "\n",
    "xgb2 = XGBRegressor(**xgb_params2)\n",
    "xgb2.fit(X_train, y_train)\n",
    "preds2 = xgb2.predict(X_test)\n",
    "\n",
    "tracker.log(\n",
    "    name='XGBoost n300 d5 lr0.05',\n",
    "    y_true=y_test, y_pred=preds2, positions=test_positions,\n",
    "    config={\n",
    "        'model': 'XGBoost',\n",
    "        'features': 'Tier 2',\n",
    "        'n_features': len(tier2_cols),\n",
    "        'train_seasons': '2022-23',\n",
    "        'test_season': '2023-24',\n",
    "        'params': xgb_params2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run 3: XGBoost deeper trees ---\n",
    "xgb_params3 = dict(n_estimators=300, max_depth=7, learning_rate=0.05,\n",
    "                   subsample=0.85, colsample_bytree=0.7, random_state=42, n_jobs=-1)\n",
    "\n",
    "xgb3 = XGBRegressor(**xgb_params3)\n",
    "xgb3.fit(X_train, y_train)\n",
    "preds3 = xgb3.predict(X_test)\n",
    "\n",
    "tracker.log(\n",
    "    name='XGBoost n300 d7 lr0.05',\n",
    "    y_true=y_test, y_pred=preds3, positions=test_positions,\n",
    "    config={\n",
    "        'model': 'XGBoost',\n",
    "        'features': 'Tier 2',\n",
    "        'n_features': len(tier2_cols),\n",
    "        'train_seasons': '2022-23',\n",
    "        'test_season': '2023-24',\n",
    "        'params': xgb_params3,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest — Experiment with Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run 1: RF default ---\n",
    "rf_params = dict(n_estimators=100, max_depth=None, min_samples_leaf=1,\n",
    "                 max_features='sqrt', random_state=42, n_jobs=-1)\n",
    "\n",
    "rf1 = RandomForestRegressor(**rf_params)\n",
    "rf1.fit(X_train, y_train)\n",
    "rf_preds1 = rf1.predict(X_test)\n",
    "\n",
    "tracker.log(\n",
    "    name='RF n100 depth=None',\n",
    "    y_true=y_test, y_pred=rf_preds1, positions=test_positions,\n",
    "    config={\n",
    "        'model': 'RandomForest',\n",
    "        'features': 'Tier 2',\n",
    "        'n_features': len(tier2_cols),\n",
    "        'train_seasons': '2022-23',\n",
    "        'test_season': '2023-24',\n",
    "        'params': rf_params,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run 2: RF regularised (max_depth, min_samples_leaf) ---\n",
    "rf_params2 = dict(n_estimators=300, max_depth=10, min_samples_leaf=3,\n",
    "                  max_features='sqrt', random_state=42, n_jobs=-1)\n",
    "\n",
    "rf2 = RandomForestRegressor(**rf_params2)\n",
    "rf2.fit(X_train, y_train)\n",
    "rf_preds2 = rf2.predict(X_test)\n",
    "\n",
    "tracker.log(\n",
    "    name='RF n300 depth=10 leaf=3',\n",
    "    y_true=y_test, y_pred=rf_preds2, positions=test_positions,\n",
    "    config={\n",
    "        'model': 'RandomForest',\n",
    "        'features': 'Tier 2',\n",
    "        'n_features': len(tier2_cols),\n",
    "        'train_seasons': '2022-23',\n",
    "        'test_season': '2023-24',\n",
    "        'params': rf_params2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results — Full Run Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE table\n",
    "tracker.summary('rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE table\n",
    "tracker.summary('mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best run by overall MAE\n",
    "best = tracker.best_run('overall_mae')\n",
    "print(f\"Best run by MAE: #{int(best['run_id'])} '{best['name']}'\")\n",
    "print(f\"  overall_rmse={best['overall_rmse']:.4f}  overall_mae={best['overall_mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper 1 benchmarks for reference\n",
    "paper1 = {\n",
    "    'run_id': '—', 'name': 'Paper1: Last5 Baseline',\n",
    "    'overall_rmse': None, 'overall_mae': None,\n",
    "    'Zeros_rmse': 0.791, 'Blanks_rmse': 1.400, 'Tickers_rmse': 2.136, 'Haulers_rmse': 5.613,\n",
    "    'Zeros_mae':  0.270, 'Blanks_mae':  0.652, 'Tickers_mae':  1.645, 'Haulers_mae':  4.709,\n",
    "}\n",
    "paper2 = {\n",
    "    'run_id': '—', 'name': 'Paper1: OpenFPL',\n",
    "    'overall_rmse': None, 'overall_mae': None,\n",
    "    'Zeros_rmse': 0.818, 'Blanks_rmse': 1.291, 'Tickers_rmse': 1.517, 'Haulers_rmse': 5.142,\n",
    "    'Zeros_mae':  0.427, 'Blanks_mae':  0.749, 'Tickers_mae':  1.127, 'Haulers_mae':  4.317,\n",
    "}\n",
    "\n",
    "all_runs = tracker.load_runs()\n",
    "paper_df = pd.DataFrame([paper1, paper2])\n",
    "\n",
    "cat_rmse_cols = ['name', 'overall_rmse', 'Zeros_rmse', 'Blanks_rmse', 'Tickers_rmse', 'Haulers_rmse']\n",
    "cat_mae_cols  = ['name', 'overall_mae',  'Zeros_mae',  'Blanks_mae',  'Tickers_mae',  'Haulers_mae']\n",
    "\n",
    "combined_rmse = pd.concat([paper_df[cat_rmse_cols], all_runs[cat_rmse_cols]], ignore_index=True)\n",
    "combined_mae  = pd.concat([paper_df[cat_mae_cols],  all_runs[cat_mae_cols]],  ignore_index=True)\n",
    "\n",
    "print('RMSE BY RETURN CATEGORY (our runs + Paper 1)')\n",
    "print(combined_rmse.to_string(index=False, float_format=lambda x: f'{x:.4f}'))\n",
    "\n",
    "print('\\nMAE BY RETURN CATEGORY (our runs + Paper 1)')\n",
    "print(combined_mae.to_string(index=False, float_format=lambda x: f'{x:.4f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE by category — all runs + Paper 1\n",
    "categories = ['Zeros', 'Blanks', 'Tickers', 'Haulers']\n",
    "all_runs = tracker.load_runs()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "for ax, metric in zip(axes, ['rmse', 'mae']):\n",
    "    cols = [f'{c}_{metric}' for c in categories]\n",
    "\n",
    "    # Paper 1 lines first\n",
    "    ax.plot(categories, [paper1[c] for c in cols], 'k--o', lw=2, label='Paper1: Last5', zorder=5)\n",
    "    ax.plot(categories, [paper2[c] for c in cols], 'k-^',  lw=2, label='Paper1: OpenFPL', zorder=5)\n",
    "\n",
    "    # Our runs\n",
    "    for _, row in all_runs.iterrows():\n",
    "        vals = [row.get(c) for c in cols]\n",
    "        ax.plot(categories, vals, '-o', lw=1.5, ms=6, label=row['name'])\n",
    "\n",
    "    ax.set_title(f'{metric.upper()} by Return Category', fontweight='bold', fontsize=13)\n",
    "    ax.set_ylabel(metric.upper())\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall RMSE / MAE comparison bar chart\n",
    "all_runs = tracker.load_runs()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, metric in zip(axes, ['overall_rmse', 'overall_mae']):\n",
    "    ax.barh(all_runs['name'], all_runs[metric], color='steelblue', alpha=0.8, edgecolor='black')\n",
    "    ax.set_title(metric.replace('_', ' ').upper(), fontweight='bold')\n",
    "    ax.set_xlabel(metric.split('_')[1].upper())\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    for i, v in enumerate(all_runs[metric]):\n",
    "        ax.text(v + 0.005, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('All Runs — Overall Metrics', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for best XGBoost run (by MAE)\n",
    "xgb_runs = all_runs[all_runs['model'] == 'XGBoost']\n",
    "best_xgb_idx = xgb_runs['overall_mae'].idxmin()\n",
    "print(f\"Best XGBoost run: #{int(all_runs.loc[best_xgb_idx, 'run_id'])} '{all_runs.loc[best_xgb_idx, 'name']}'\")\n",
    "\n",
    "# Use the last trained XGBoost (you may need to retrain if restarting the kernel)\n",
    "# xgb2 = best based on the runs above — adjust if needed\n",
    "best_xgb = xgb2   # change this if a different run is best\n",
    "\n",
    "imp = pd.Series(best_xgb.feature_importances_, index=tier2_cols).sort_values()\n",
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "imp.plot(kind='barh', ax=ax, color='orange', alpha=0.8, edgecolor='black')\n",
    "ax.set_title('XGBoost Feature Importance (best run)', fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load All Historical Runs\n",
    "\n",
    "Runs accumulate across notebook sessions. Run this cell any time to see everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import load_runs\n",
    "\n",
    "all_runs_df = load_runs()\n",
    "print(f'Total runs logged so far: {len(all_runs_df)}')\n",
    "all_runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean pivot: runs × metrics (useful for sharing / exporting)\n",
    "pivot = tracker.comparison_table('rmse')\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to reset and start fresh (deletes all logged runs!)\n",
    "# tracker.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
